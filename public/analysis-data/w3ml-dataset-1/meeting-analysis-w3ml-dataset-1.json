{
  "meeting_id": "w3ml-dataset-1",
  "meeting_title": "Recording",
  "date": "2025-03-25T00:00:08+00:00",
  "platform": "Zoom",
  "metadata": {
    "source_file": "GMT20250325-000008_Recording.cutfile.20250325053526279.transcript.vtt",
    "file_type": "vtt",
    "engagement_score": 74.11764705882352
  },
  "transcript": "Hai: I know Mary has some special announcements to make Meri Nova: I think I already did. Oh, wait! Wait! Wait! I didn't. I didn't. So let's see. Meri Nova: we just wanted to like say that from today would be a nice idea to highlight a builder of the week or something. Meri Nova: We can do it randomly. So it's not gonna be like. At least that's what I thought. Hi, let me know if you have other ideas. But we're just gonna randomly highlight. A person that Meri Nova: had probably struggled in the beginning and right now is doing much better. And I feel like it's a great example for others, too, to get inspired. And for this week, Hi, who are we highlighting for this week Hai: So this is supposed to be very special, because we've been watching you guys for a couple of weeks. Hai: When I say couple of weeks. Anything. What I meant is what 3 weeks Hai: right? It's been 3 weeks right Meri Nova: 2 weeks. It's been 2 weeks Hai: Okay, 2 weeks. Hai: and everyone's going through a different journey right? Some people hit the ground running really fast, others took their time to think about what they wanted to build. Hai: And a lot of you guys run into things that you've never seen before. And for the 1st time in your life. You have to debug Hai: stuff built by AI, Hai: and I don't know how you feel about that. But I think from our perspective, it's very inspirational to see somebody going from 0 to like a hundred just vibe coding and then finding their way out of bugs. So we want to dedicate once a week. We want to dedicate like Hai: an announcement to some of our students who we think are Hai: such an inspiration for everyone here. Hai: So we have. We have a list. Hai: and Mary's going to say it Meri Nova: With that you had a list, or you were picking one person Hai: I said, you have the list. Meri Nova: I do, just because it was so hard to pick one person, and I feel like that would be unfair for everybody else. But like from the top of my head. I just wanted to let you know that some people are also reaching out to me in the Dms. As well, not just in the general chat, especially those who are like confused or scared, or just overwhelmed Hai: Weird. Hai: What did I do? Just kidding Meri Nova: You didn't do nothing. You're doing great. No, but for those that were so, I just wanted to. 1st of all, say, like Praveena, I just want to give a shout out to Praveena, because we had a conversation before, and I could resonate with like the feelings of overwhelm, and I feel like the 1st few weeks were really tough for many of us, because we basically put you in the middle of the fire and said, you got to figure out. Meri Nova: and I'm just like so proud of for those who have never coded before and stuck with us with us for like 2 weeks now, and I just want to congratulate Praveena. Meri Nova: We're doing great and setting up the login page, even though it was hard to figure out. Meri Nova: And I have another second person that I think is worth highlighting. Hi, feel free to like add more people, too. But these are just like from the top of my head. Everyone. It doesn't mean that everybody else is not doing great. In fact, there are so many amazing people. But I know that you guys are already doing great. I mean, you're already like an a-list. But I'm talking about people who have been struggling a little bit, and I just want to give a shout out Meri Nova: to those people. And second person I had in mind was Kelsey. I just think that you're doing amazing, Kelsey. Meri Nova: You did a great like, I know you were struggling with your data set. And then you said, Wait a minute, let me regroup. Let me see what's important. And then you completed the thing, and you even put on the demo. So I think that was awesome. So would love to highlight more people more like stories like these so feel free to share, like Meri Nova: your struggles, or whatever you're feeling in the DM. Or even general Chat, because later on you're going to overcome them, and we're going to celebrate you. So I just, I just want to like, say, congratulations, guys, let's let's keep building Praveena Suresh: Thank thank you, Mary. That's very kind of you. Thank you. Meri Nova: Great job for Rena. Yeah. kelseydilullo: Yeah, thank you so much. Appreciate the kind words Meri Nova: Awesome happy to hear this. Meri Nova: Great. Meri Nova: I think that's all I wanted to say, Hi, unless you had more people to highlight Hai: No, I don't. But today we're going to try a new drawing tool called thick jam. Hai: Does anyone here know what figjam is? Hai: Nobody Frederick Z: Is it similar to figma? Because I've heard of figma Hai: Yeah, it is a tool from figma. Anyone here knows what figma is. Hai: Yep, anyone here had. Okay, I see some nods. Hai: So Figma, let me just share my screen real quick. Hai: Oh, oh, but a great segue! Hai: So figma is a design tool. Hai: It's a tool for designers to create. Hai: you know mock-ups and stuff like that, for, you know, to working in a development team. So designer can work with developers. And now you can even make a design and then export it as code. Hai: which is really cool. Hai: Side fun. Fact. My sister works at Figma. Hai: So I'm supposed to be like the cool, techie, little brother. But then my sister has Hai: more interesting job. So it's like, it's like a rivalry that never ends. Unfortunately. Hai: But I'm going to send out this link for you guys so that we can all hop on this fig jam together. Hai: Let me drop this in the chat real quick Meri Nova: Is it gonna help Hai: Okay. Meri Nova: Space for all of us. Hai: Oh, yeah. Hai: okay, I'm seeing orange so fast that I just dropped this thing in. Hai: Hello, everybody. Hai: When I was at my last job. Whenever we're on like a fake jam with like 20 or 30 people or so, you have to pay really close attention, so that your cursor doesn't touch someone else's cursor without their consent. Otherwise everyone gets into trouble. Hai: Okay, everybody. Hai: Today, we're going to be talking about Hai: arguably, one of the most important foundation towards understanding agents. It's a couple of topics we're going to talk about. One of them is tool calling and the other one is structured outputs. Hai: So let me just write that out here real quick. One is tool calling. Hai: And then I like how everyone, just all of a sudden just stops when I start typing. The other one is structured Hai: outputs. Hai: Okay? Hai: So I have a question for everyone here. Hai: Oh, there's a table. Hold on! Hold on! Hai: There we go! That's that's way. Better. Hai: We have an upgrade guys. This is not free Excali draw silliness. This is multi-billion dollar Hai: company's tool. Okay, so Hai: tool calling and structure outputs, I'm sure, for some of you guys, these are familiar terms. So Hai: does anyone want to take a jab at explaining Hai: tool? What tool calling is on the call Sagar: So tool call is to my understanding, is like a Plugin to the Llm. To do the work that by default Llm. May not be able to do it. Structured output would be something to have a standard response structure so that you can parse it like a Json or something. Sagar: So that you can parse the output and do the next steps Hai: Okay, I'm just gonna correct that 1st part that you said there real quick. Hai: So for tool calling. Hai: What it means is that you. It's it's a large sandwich model plus an application. So the large sandwich model is taken in an input. Hai: and it's basically fine-tuned or pre-fit in a way. So that Hai: if the input has a list of tools on the output, it will output the tool that is the most relevant for the conversation. Hai: That's basically it. So for example, I say to an Llm that, hey, like you are an expert at. Hai: you know. Hai: a couple of different things. You have access to a tool that get you the weather. You have access to a tool that get you the stock price. And the next message I have for it is. What is the current stock price for Tesla? I know it's going to go. It's been going down for a while, so I'm like, what's the stock price for Tesla. Hai: Large language models nowadays is smart enough to know that, hey? I'm asking for a particular tool, and it's supposed to choose a tool as long as I put that little prompt in my system message that says you are an expert at choosing tool, you have access to these tools, and you only output the tool Hai: so like you, only so realistically, you only have to worry about prompting it to use tools. Hai: If you're using an open source model. If you're using a model that is like. Hai: kind of have no infrastructure around it. So kind of like a llama 3.3. That is, you setting up Hai: from scratch. But if you're hitting like Openai, if you're hitting like any frontier models right now, like Hai: cohere or anthropic. Hai: you usually have a parameter that allows you to turn on this thing called tool calling, and then you're passing a list of tools, and then they will do that for you and Sargai was right. That for tool calling Hai: you now move from an Llm. Doing just, you know, text generation to now. Lm, deciding what code to run or what functions to run. Hai: So that's basically it. So does anyone want to like Hai: take a jab at recapping what I just said, just to see if you like, understood Hai: what tool calling is, and you can say it wrong. I don't care. I'll tell you Autumn Hicks: Yes. Hai: Nothing wrong with that. Yeah. Autumn Hicks: Sure. So utilizing external Apis and other other tools with an Llm. To call on them. Is, am I getting that correct? Hai: Yeah, that's pretty much it, and not, you know, not constrained to just 3rd party. Apis, these are literally just like Hai: code function. So like when you write like, you know, a function in Javascript like that's 1 of the use cases. So go ahead and write that under tool calling like the definition that you just said Meri Nova: We got to write things Hai: Yeah, I'm like getting lazier and lazier every week. So. Hai: starting today, I'm not writing the tables. Next week, I'll I'll let somebody else do the presentation Meri Nova: Oh, my God! I don't think there's permission, though, to write Hai: Okay, approve. Hai: Let's see, or hmm, was this in? Let's see if this Hai: okay, can you guys right now Meri Nova: Let's see. Oren: Nope, it won't work Hai: So anticlimactic hold on can view, can edit, can demetrios dolios: But yeah, double click inside and start typing. I I was able to Hai: And if you demetrios dolios: Though Hai: Okay, can you guys edit it? Now. Meri Nova: Yeah, you're right? Hai: Alright, I think autumn is. It's autumn's turn Autumn Hicks: Okay, yeah, I think I have access. Hang on Meri Nova: I think we need to do. There's on the panel on the bottom. It says, View. Only you need to ask to edit, and then it automatically approves you. Meri Nova: That's what happened to me. Hai: I think I made it so anyone can edit now. Hai: maybe you need to refresh or something Meri Nova: Yeah. Refresh. Hai: Do, do! Do! Hai: Oh, somebody's typing something Meri Nova: Dwayne, that's not your turn. Autumn Hicks: It's working now. Okay, hang on Dwayne Joseph: Dwayne didn't type anything. Meri Nova: Oh! Autumn Hicks: That was me Meri Nova: Okay. Okay. Hai: Doing something. Autumn Hicks: I'm trying to think of another word other than just tools. Autumn Hicks: Hello, the plus tools. There we go. Thank you. Hai: Yeah, so it could be external functions, internal functions like internal in terms of like inside of your application. Hai: Yeah. So while we're doing that. Let me pull up the AI SDK, Hai: and I'll show you guys what Hai: how you can do that in in here, so Hai: you you can go into the documentation. You can look up tool calling. Hai: Let's see tool calling. Hai: Yeah. So this right here Hai: tools are objects that can be called by the model to perform a specific task. AI SDK Core contains 3 elements, description parameters execute. Hai: Okay? So Hai: now that we know that when you give a list of tools to a large language model and tell it that. Hey, you can use these tools. It will return back the tool name not only that it can also return back the parameters or the arguments that can go inside the function. Hai: So, for example, I have like get weather function, and inside of the get weather function. I have like an argument, for, like city name, the large language model is smart enough to that to look at my query and be like, Hey, okay, this person is asking about like Hai: San Francisco. So I'm going to extract that and then throw that into the tool call and then run, run the tool with the new parameter. Hai: So let's take a look at the the sample code here. Hai: So here, if you're using like nextjs and typescript. And you're using AI SDK, what you'll see is you can basically use the same method as when you were generating like regular text. It still generate text, and then you pass in like your Gpt. 4. 0, whatever. Hai: But then you have this like property that you can pass in this property called tools, and then in here. What you can do is you can pass in like, you know, different tools. So the way you do that is, you create a property name, and then the property value Hai: so it could be weather. It could be like stock whatever. As long as you don't have like a space between, and then you use this thing called tool, which you import from from the AI SDK to define the actual tool that the AI can use. And this is really funny, because this is basically giving it to the large language model. Which means that the tool name actually matters. So you want the tool to do like Hai: get information from or get information from web search. Or, yeah, maybe the tool is called web search. Then you need to call it the web search. You can't just call it like Xyz, because that affects the decision-making process from the Lm. Hai: And as your application gets a little more complex, what you really want to do is not only have a good name for your tool, but also have a good description. Hai: The description tells the Llm. When to use it, and you feel free to like in the description, give it examples of when to use it. So, for example, let's say a tool. Your application has a tool for the Llm. To do web search, tell it when to use it. So like, tell it like, Okay, so if you're looking at their messages, history. You're looking at Hai: the retrieval, and you don't see enough information to answer the user's question. You must use a web search tool, so that in that case it'll trigger the web search tool when the situation calls for it. Hai: yeah. And then there are 2 ways to like. This goes for both python people and and Nextjs people. There are 2 ways to do what you would do with the tool after it's been chosen. You could either Hai: choose like either. Just look at the result as it is, because right now, what happens is, it's gonna like, just give you like Hai: the weather tool, as like, hey? Based on our conversation. I have chosen this tool, but using a wrapper library like the AI SDK, allows you to execute the tool as well. So before you had to like, parse it and say, okay. So because it chose Hai: web search, I have to run this function. But here you have a callback that you can pass in here. So let's say, your web search tool is tivili. This is where you pass in your tivili function. Hai: and then it'll get executed on the front end or the back end depending on like, what kind of application you're building? Hai: And yeah. And then it gets more complicated than that. So so far Hai: does do any of these things make sense to you guys on like a high level Hai: terms of tool calling. Hai: If you have questions just ask me right now. Sagar: So generally. In the prompt itself we specify right like. Sagar: If you want to use this, if you if you want to fetch weather. Then use this tool, or something like that. Is that not the case? Here? Hai: Yeah, you should absolutely do that. You should actually do that in multiple places. And speaking from experience. So you have the system message. Hai: That's why you should say it. You should say that you have access to these 5, 10 different tools. Each one does. XYZ. And then you specify again in the description, and then you specify again in the parameters. So the parameters would be like, Okay, like, if Hai: Xyz is available, then you got to insert that. Hai: So the parameter goes inside of the function. Right? Like function has arguments. Those are just parameters. Yeah. So you have to specify a couple of different places. So the system message, and then the description of the tool. Hai: and then, in the description of the tool, feel free to give it examples Hai: colloquially. We call this a few shot, even though it's not technically Hai: fine-tuning or pre-training. Few shots just mean, like, you know, you give it. You give the a model like Hai: a couple of different examples, so that it knows when to do something better. Hai: How does that sound Sagar? Sagar: Yep, yep, I was also wondering. Is this the like similar to AI SDK? Is is this provision also available through any other frameworks, or like through open AI's SDK itself. Do? Can we do that as well Hai: Yeah, so it gets a little bit more complicated than that. And I'll explain it right here. Hai: Everybody gather around. Let me explain this. Hai: Okay, we got the Lm tool calling is an Llm thing. Hai: The infrastructure around it, like the AI SDK is just the software engineering stuff around it to make your life easier. Hai: So the Lm should be fine tune. Hai: Hold on, new tool. How do I? What's the best way to do this? Actually? Hai: Let's see. Hai: I know I have sticky notes in here. Let me try find. Oh, there, there they are, sticky Hai: boom. There you go. Hai: Okay, we got. We got Lms. Hai: so has to be fine-tuned to use tools, otherwise low performance. Hai: And we see this from models like Gpt-three. So Gpt-three was able to use tools, quote unquote, but it was terrible at it. Hai: like you have to tell it like 20 times when to use what, in order for it to be able to pick the right tool Hai: later, stage models like starting with Gpt-four. And this is actually the reason why Gpt-four kind of kickstart the whole agent stuff back in 2023 like before Gpt-four. Like, it's just really hard to build agents. Gpt-four was sort of fine-tuned to to be able to use tools when given the opportunity to use tools, and it was able to like pick tools based on, like the situation of the conversation, very situationally. Hai: So this is a very Llm. Thing like we haven't talked about the software part yet, and this is not AI SDK, not like Llm. Not not anything like that. Hai: If you have an Llm. And if you just run it on your computer and you tell it like, Oh, there are tools you can use. It can do this as long as you prompt it, really? Well, and that's why Hai: sdks like, or libraries like libraries like Langchain. Hai: AI SDK I don't know if Lm. Does this, but I know but pydantic AI, or you know, Openai agents, SDK, they all have built in prompts Hai: built in proms for tool use. Hai: The reason for that. This is because Hai: if you were to go and try to prompt a model to use tool today, you're not going to get that great of an experience compared to somebody who has been doing this for a while. And this is the same reason why libraries like Langchain was created, which is because there are patterns that have been repeated over and over. So people just kind of build a library around it. And then. Hai: now you just have this pattern that you can just plug in. So whenever you declare a tool in Langchain, there's a whole set of problems behind that that says you are blah blah, you can use this tool and blah blah blah, but you have to click through a bunch of stuff to see it, because it's in the library's code. It's not visible to you all, you see, as a user of Langchain or as a user of AI SDK, all you see is that, oh, I can just Hai: put like a little like Hai: tool thing here, and they'll just work. But there's like a whole bunch of like hidden prompts behind it. Hai: And this is all like prompting stuff, and you'll see like, you know, if you pass in a super small model, it'll do terribly, and you pass in a more modern and recent model like Gpt. 4. 0, it will do really well. Hai: because it's the same prompts behind the scenes. But then is the model smart enough to even use tool that kind of stuff. So Hai: if you end up using like any of the frontier models today, and you use like a library like AI SDK, or lightly Lm, or Langchain. Hai: you'll likely you you had like, like Hai: the probably the most consistent experience with like Hai: using tools, you'll be fine, basically, unless you're trying to push the boundaries and like, give it like a bunch of tools. Or your use case is like kind of niche. Then you might need to like optimize stuff here and there. But Hai: yeah, so that's that's the behind the scenes. So you need an Llm that knows how to call tools. You need a set of like infrastructure software infrastructure around it so that it can, like, you know, create tools and in a clean code way. And then, like potentially be able to execute it in a clean way. Hai: That's why we use these SDK, so that you know, this could have been like 20, not 25 times more amount of code if you don't use like a library like this to set up these tool calling stuff. Yes, I hope that makes sense. Why, you know. Hai: AI SDK, and stuff like that exist, and why they work with tool calling. And all these things Frederick Z: Hey? Hi! By by any chance. Do you know of any open source? Models that work pretty well? That work like at least semi well with tools, or or for agent Hai: Yeah. Quan models do really? Well, I know Deep Seat does decently as well anything that's came out recently. They all have like tool calling baked in. Because this is like I said in the beginning, this is the foundation of knowing how to run agents. Hai: The reason why agents is super useful is because they can interact with applications. And 3rd party external Apis, right all through tool calling. Hai: But Hai: I want to like, introduce you guys to this. The other thing as well. Let's go, agent, small agent. Hai: So there are 2 school of thoughts to this right? There's a school of thought that says, Hey, if we just lay out all the tools to. And if you think about Mcp, Mcp is basically tool calling. By the way, there's a server that has a bunch of tools and a description of when to use these tools, and then your agent is kind of like, oh, I'm reading all these description, and I have this task I need to do. I'll just call the tool that I need. Hai: That's what this whole Mcp thing is about Hai: tools. And then your agent goes to the tools, and like, I'll read all this, and I'll run the thing that I need. But there's 2 school thoughts. It's going back to figjam here. Hai: The tool school of thoughts. You either do tool calling. Hai: Let's see you either do tool calling or you do code generation. Hai: So there are a couple of papers that came out Hai: not recently. There are some that came out recently, but argues that you get better performance when running like Hai: like the so the end goal, the end goal end goal is to run Hai: Co to run a process of some kind, maybe like call external Api, or, you know. Hai: change a front end thing. Hai: that kind of stuff. So either of these would help you accomplish that. But the more beginner friendly stuff is tool calling. Hai: And some people argue that, hey? Like, you know, if you allow the model to generate code based on the specs. So the specs is just like, you know the description of like the tools stuff like that. Hai: You get better performance. So like, instead of writing like, Hey, you have access to Getwether and get stocks. Hai: It lets them. They let the model write out what get stocks is in python, or they'll let the model write out like, get weather in python. Hai: And then they argued that because there's more flexibility there, the model can write the right function to call the right endpoint instead of like being, you know, constrained in the function that you wrote for the model. Hai: But you don't have to worry about this right now. Just know that there are 2 school of thoughts. Out there. One is is, let the model write code in a sandbox, and 2 is, you only have access to these tools and just run either one. Hai: Yeah. Hai: But the end goal is always. I need to like, do something. Hai: And that's why, you know, next week we're going to talk about Asians and Asians require this knowledge for you guys to understand it? Hai: How does that sound like, does this tool calling make sense to everybody? Hai: Okay, okay, is anyone using doing tool calling right now in in their projects? Frederick Z: I am, and I'm using Quinn. I tried using Deep seat, but I think that there's some sort of incompatibility issue with lang graph and deep seat, because when I tried it, it says, like deep seat, does not support tool calling, which I found surprising. But Hai: Yeah. But did I tell you about like the hidden problems? Langchain is just full of those problems. You gotta like, just click, click, click, click, click, and you'll see like some some like half baked Prom, that doesn't work for deep sea Frederick Z: I'm going to implement your idea about like inserting like, tool calling prompts in in multiple places. So so there's the system prompt that there's the prompt for the agent and solve it. And you you also said for the description. Can you remind me of real quick where where the description goes Hai: Yeah. System, message for the for the agent, or like for your Lm, your router, or whatever. And then the description, the name of the tool also needs to be very descriptive. Hai: For example, if you have a tool that calls your Crm like this is a call it like tool to update contact. And Crm like, that's something has to be very descriptive like that. And then you go to the description. Here you got to be very specific about the tool again, and then you give examples of when to run the tool, and then in the parameters make sure that you know whatever arguments you have for the function Hai: again, reiterate and like, just say, like, this is when to run it. And then this is how you get the information from the conversation history. Hai: Yeah. So she's saying, like, just kind of like, repeat yourself in the description and kind of repeat a little bit more in the parameters. Hai: and, like the system message for the agent Frederick Z: Okay, cool. Thanks. Thanks. I'm gonna try that. Hai: Yeah, anyone else. Hai: I think I saw some nods. People using tool calling. If you want to like, give an example of what you're using it for feel free to unmute yourself and say it. Hai: Now everyone decides to be shy. Huh! I saw some some nods. I'll point. I'll point you out if you don't Meri Nova: That's a good idea. Actually feel free to point out put people on the spot. But I could share my use case Meri Nova: So Meri Nova: I'm building an agent to research some topic that the user gives. And one of the. So I I'm assuming I'll have 2 tools. One of them is a web search and another one is the writer tool. Meri Nova: And I'm using openai SDK agents. Actually, after we had a conversation about which which agent framework will last. But so Meri Nova: it's it's interesting library, actually, because you can use agents as tools as well. But anyway. So the the context is also interesting how you should pass context between those tools as well, because mine depends. The second, the writer technical tool depends on the context of the 1st tool gives me from the web search Meri Nova: So, yeah, that's the use case. Hai: Nice. That's a yeah. That's a use case right there. Hai: Anyone else Sagar: Honestly, I'm not in the way that you just explained, but I'm kind of making a function call. Sagar: but I would prefer to make it. Now. Sagar: the way you explained like instead of like. Sagar: right now, it's more of like a workflow for me where? Sagar: So my agent is. Basically reviews. Pull requests. Sagar: Give some suggestions on the get on the pull request, and then Sagar: there's another agent which reads through those comments and incorporates those changes automatically into the code. Sagar: So there's a there. Mostly my tools are revolved around Github Apis, where I would need to Sagar: fetch some details from the pull request code Sagar: or some tools would be around getting parsing the code and getting some relevant function or class to get some context or something like around that. So Sagar: at the moment. It's like a workflow, which is very. Sagar: very controlled the way I want. Sagar: It's not like orchestrated as such that, you know. Hey, Llm, call this whenever you need have a need, so there's no con. I feel that it's more of like orchestrated flow, not like Hai: I wouldn't. I wouldn't change a thing about it. The reason for that is because Hai: forcing the model to call only one tool is actually a use case. Hai: And that's why Openai has this thing called force tool calling. Hai: Because tool calling is like a really great way to Hai: extract parameters. People realized. If you don't do that, and you say that, hey? Like, I just want it to like Hai: extract like a list of stuff, and then and then you like heuristically pass those values to like a real function, to call after that it's actually not as performant as like, hey, you have. It's not as performant as telling the model. Hey, you have access to one tool, and this tool takes these arguments. Hai: find the arguments for me from the from the conversation so Hai: function calling. That's why they made it so that Sagar: There's an option so that Hai: So Hai: by default. A model from Openai, for example, will not call a tool. If the situation doesn't require it to call a tool, it will actually go, you know, tell you in, in like Hai: like natural language, just kind of continue the conversation. So, for example. Hai: you tell it, let you tell it that you have access to like 2 tools. One is, get weather, the other one is stock prices. And then you say, like, what? Who's like the Hai: 40th President of the United States, and it will actually, like most of the time, for, like a open AI model, it'll just respond back in natural language, just like how you you would otherwise not have any tools given to it. Hai: But you can have this thing where you can, you know, force a 2 call so that it'll always like, get weather, or always like get stock prices. So if you ask that question. Hai: it'll actually come up with some weird stuff to like, pass into one of those functions, to like force, a skit weather, or, like force, get stock prices. Hai: which is, turns out to be very useful in Hai: real world scenarios, because a lot of times the conversation usually hints towards something and user doesn't really like explicitly ask for it. Hai: So in cases, like, you know, hey? Like, you know. Hai: I'm I'm asking the model some questions. I'm asking the the AI some questions, and it doesn't have access to like a bunch of stuff. Hai: But sometimes models are not smart enough to go online unless you say, now, go online and search for stuff. Then you know, the tool tool hit rate is way higher. Hai: But sometimes we want the model to be like Hai: smart enough to know that, hey? Like we're talking in circle here and just go online search for stuff. Hai: And that's when, like forced tool calling comes in Hai: and that's why, like, there's some like really interesting patterns where you can do stuff like you have a tool. So you have, like a like a AI Chatbot, or whatever right? Hai: And then this chat bot has, like web, search tool. Hai: And then Hai: what people realize that this is what I do as well is that you can basically let the Chatbot have more normal conversation with you while having a web search tool by wrapping another tool called Hai: I don't know, conversate. Hai: And then this would actually like. Hai: whenever based on the situation of the conversation. What was happening in the messages? History? Hai: Usually it'll know when to call conversation and just talk like a normal Chatbot, and then, when when to like, call the the web search tool. Hai: And then there are some models that you can do, web search and then compensate in multiple tools in different steps as well, which, if you want to ask me about it, I can show you Hai: So this is like apparently more performance than just having the web search tool Hai: and then force it to like talk when it doesn't need to. Hai: For whatever reason, like these models are not, you know, we're just trying to figure out what to do with these these models. So Hai: that is from what I've known like. If you have like this all of a sudden. Now it's like Hai: good at talking, and also decent at using tools. Hai: But if you don't have it now, it's like, don't know when to talk and don't know when to use tools. Hai: So there's a pattern where you like, you can basically constrain whatever the model is trying to do into like tools. So like, if you wanted to talk, you have a talk tool. If you wanted to web search, have a web search tool, and if we want it to like. Hai: I don't know, reiterate, or like Hai: talk to a certain way. You have another tool, so you can even have stuff like, you know. Hai: conversate. And then, you know, conversate like a pirate or something. Hai: So I think, basically like tool calling just means that, hey? Like you are allowed to do these things Hai: do the thing, do the right thing at the right time, basically. Hai: And yeah, I hope this isn't doesn't confuse you guys too much. Hai: Okay, Oren's like nodding. So I guess this is very confusing. Hai: Just a little bit confusing. Okay, cool. Hai: yeah. So cold calling gets pretty deep. We'll talk more about it in in like our agent stuff next week. Hai: and I know some of you guys are already doing tool calling so cigar like I wouldn't change a thing about it. Hai: but about your workflow, because Hai: if it's supposed to be a workflow, then it's supposed to be a workflow. Don't make it like chain of thought and stuff, and then it will quote the wrong tools. And then you have, like the worst performing workflow. Hai: I mean, that's just my 2 cents Sagar: Yep. Hai: Okay structure upwards. Anyone wants to take a jab. At this Hai: I saw Duane unmuting, and then muted himself again. Hai: This man changed his mind Dwayne Joseph: I was gonna say, oh, go ahead! Go ahead! Athira Praveen: Oh, sorry I was Athira Praveen: thinking about the tool calling again, and had a quick question. So is the just to reiterate is the goal of having these tools is to have constraints around what the Lm can do like, give it guard guardrails to what it can do rather than, for example, in my project, it's to retrieve Athira Praveen: information right? So instead of just giving it to Aist, can be like, Oh, retrieve all this for me. I say that you are this, and you can retrieve so and so information based on this and this. So I have a tool around that. Athira Praveen: except that Hai: Pretty much. And then if your AI needs to do something else, you just wrap that inside another tool Athira Praveen: Yeah, another tool could be like an Lm call Hai: Like it could be like just a generate text call like this Hai: with like a no tools. And just like, here's the message's history, and just continue the conversation. The tool can be called, continue the conversation. Hai: and now it cannot do anything outside of like these 2 things, which is a great thing to do with tool calling Hai: and you're absolutely right. People use tool calling to do gut rails as well. I do that as well. Tool calling is just a way to like force, the model to be like, constrained into doing certain things, and not anything outside of that Hai: which is great, because then I can always have a tool in my router. So my router is kind of like, basically an Lm call that says, like, you have Hai: a couple of options to continue the conversation. Hai: and one of one of my tools would be to reject the user's request. Hai: And then I have a description for that that says like, Hey, we should find this tool when the user is asking about something inappropriate. Hai: then that supposedly like make it more performant when it sees like a request that comes in that is not appropriate, and then it will actually choose the tool as opposed to like. Try to reject that by sending back like, hey? As an AI. Oh, I can't do that, or whatever Hai: in that sense you can always also control what what you send back. Hai: And then there's a pattern where you can say like. Hai: I can wrap these. I can wrap canned responses in tools as well. So if you have like a line that you always tell people when they're doing something inappropriate. You can wrap that like you can hard code that into a string and then wrap that inside a tool that says respond to inappropriate requests so that you can say, like somebody says something inappropriate. They'll run that tool, and then you'll always send back like a particular response that you yourself have chosen Hai: like, because sometimes you don't want it to say like as an AI right, sometimes you want it to say like, Oh, you should contact administrator, or whatever. So that's now under your control, right? Athira Praveen: That makes sense. Thank you. Hai: Yeah. Hai: Okay, Dwayne, you wanna do Json mode real quick or structure output Dwayne Joseph: by the name of it. I was gonna kind of guess that the output so it's a way to make sure that the output that the model gives you is in a specific structure or specific format that was, gonna do my guess Hai: Pretty much even though we have the table like this. But structure outputs predate tool calling Hai: because we had structure outputs, which is, you know, if you if you tell like a model, hey? Like Return Json, so that you know Json is more friendly to your database format Hai: back in like 2023, 2022. It was really hard to do that. The model would like return Hai: Broken Json with, like, you know, maybe maybe it's missing like a curly bracket at the end. Or maybe, like properties are wrong stuff like that. But nowadays most frontier models can do this really well. Hai: Structure outputs just mean, hey? If you want to give me Json, so that it's now structured data for me to use in my either calling the next function or saving this data into my database. Hai: That's pretty much it. The structure outputs Hai: If you use something like Langchain Hai: again, there's a hidden prompt inside of like the land chain structured output. Hai: Function. Hai: Oh, hold on, let me find the latest one. Hai: Yeah. So this right here with structure output. There's like a promise behind that. That says Hai: you are a Json expert, or whatever like. You only return, Json, and then say that 10 times. Hai: and then hope that the model will actually do that. Hai: There's a problem with, like, you know. Hai: getting them trying to get the model to return a structured output, like, you know, kind of a json like this. Hai: or Json like this, and it always returns like, sometimes I return with with that like this curly bracket here, or something wrong, maybe, like a missing quote right here. So what we what some companies do is that they introduced fine-tuned models, models that are fine-tuned to just do Json. So Openai has one. Gemini has one. Hai: So when I say Gemini, I mean, if you go to Google AI studio and then you're trying to like run a query. What you can do is you can go to the side panel here, as you can see, they call it function calling, but it's actually tool calling Hai: just the same thing. Hai: But structure output right here, so you can turn this on, and then you can edit it. Hai: Usually you want to give it like a some sort of schema, so that you can follow. Hai: So, for example, if you want the output to be like, I want it to look like this, and then you know these properties, these keys are, you know. Hai: what is Orin laughing about Oren: I was struggling with the structured outputs for 2 days now Hai: I wanted to realize it's right there. Hai: What? Which model are you using Oren: And terminate Hai: Dave and I, okay, so did you. Did you give it a schema? Oren: Not yet, but I was walking on it like half an hour before we started this, and I'm gonna walk on it now. Hai: Ok. So the easiest way to do a schema here is you can code out your Json here, but I'm severely dyslexic, so I don't know when to tap stuff. Hai: So what I do is I usually go to the visual editor, and now I can like start adding properties. So, for example, if your use case, like, I'm looking at businesses from my web search, I need to. So I'm scraping a website. And then I need just the structure output of like the business like, you know, business, name, business, id whatever. So I can type it out like this business name. Hai: There should be a string, and then you can mark something as required or not required. So mark that was required, and then maybe, like business address Hai: optional. And then, now, if I go back to this, I get a pretty pretty nice Hai: Json schema. Hai: So this is basically tells the model, especially the fine-tuned models from from Openai or Gemini Hai: that hey? This is the the format that I want at the end, like you need to follow this. Hai: and then, if you don't follow this, you will be fined 200 bucks that that might actually work in the prong. Hai: Try that. But yeah. So the model will look at this and be like, okay, I'll try my best to generate something that adheres to this kind of schema. Hai: So easiest way to do it through Gemini is to go on Google AI studio. Click, click, click, right here, type some stuff. And then, you know, you have a Json thing here. Openai has a better thing. And this is what I ran into in my hackathon. I gotta tell you guys Hai: so what I was doing. The hackathon does anyone here try like the project that I built. Hai: I know some people did. Hai: But the tool, basically. What it does is it looks at a code base, and then it tries to generate a bunch of steps on how to recreate a feature from that code base. So that's basically a structured output problem. I want a list of objects Hai: from an input string, which is basically the entire code base concatenated into one big string. Hai: So I try to use Gemini. Hai: And what happened was because the code input was so long that by the time it tries to generate the Json it always generated without like missing stuff. And then it turns out that Json, at the end was broken. It couldn't be parsed by regular code. So that's the key. If the Json couldn't be parsed by regular code at the end. That means the Json is broken, and you need to like, refine how you do structure output. Hai: So what I did was, I knew that Openai has this thing called strict mode, which means that Hai: so they advertise this mode as Hai: you get Json 100% of the time guaranteed. Hai: Strict mode. Hai: So, for example, we have a similar situation here where we declare like, you know. Hai: So we have a tool here, and then we declare it like the schema for stuff. And then actually, this is just tool calling a lot. Hai: and it's strip me out Hai: strict, strict, strict. There we go, so strict mode. Hai: So we we do we still declare like a you know. Hai: like a Json object, kind of like how we what we saw in Gemini. Hai: but in Openai their Api has this thing called strict, which means that it will guarantee that a valid Json will get sent back to you Hai: 100% of the time. Hai: If I'll get into the caveats of that, but you guys probably won't see it. But this makes our lives much easier, because then you don't have to like catch errors for Json, which is ridiculous, right like sometimes it's missing this, sometimes it's missing that, and then the whole thing breaks. But here you can always rely on it. Hai: providing you a Json. Now, if the Json is valid or not, I mean the Json is useful or not. That's another story. But you always get a json that doesn't break your code Hai: using this thing called strict mode. Hai: So, for example. Hai: so the caveat of this is that it's slower. So it takes about twice the amount of time to generate, and the reason for that is because when you turn on streak mode, your request is now hitting a different Api. Hai: And this Api has both a fine tune model that does Json and some heuristic code to kind of like error and backtrack and all that stuff so that your Json will come out to you always valid. So if you want like, you know, kind of worry, free Json schema structure output, you could always look at strict mode right here. Hai: If you disable streak mode, it's faster. But then there's a there's a 10% chance that it'll be broken, but if you turn on streak mode it will always be valid. Hai: If you, for example, for Gemini. Hai: if I were to stick with Gemini for my hackathon project. I would have to Hai: just coat, just, you know, regular coat right here, kind of like like this. Hai: you know. Run the call to Gemini. Blah blah! Get the json. But then at the end I have to like check whether or not it's a valid Json, and if it's not a valid Json, I have to take that error message Hai: and then type it right back to the model to be like, Hey, fix this, and then hopefully, the second time around it will be able to fix it. Hai: Yeah. So how does that sound for everybody? This is structured output use cases. Use cases. Hai: it's just scraping a website and trying to find like, if the website has like 10 businesses on it. And you don't want this string format because you can't save that to your database you can do structure, output to be like, hey, I want all this to be a list, and each item in a list is an object. Hai: Now you have a clean list to like, say, to save to your database instead of like one long website, I've been scraped. So that's 1. Another thing is, you know, you're parsing long documents, and you're like, Oh, I'm just looking for like this particular information piece of information, or I'm just looking for like 3 key information. And you know, you can create a schema that says something like Hai: address, maybe like somebody sent you like an email. And you just wanted the the email address. Hai: And then you want, like, you know, I don't know. This is kind of like reaching but actionable items. Hai: and this could be like an array of string or something like that. Then, you know, you would have an easier time parsing it if it comes back as Json. Hai: and the the whole idea of like a structure output is that it's very friendly to your code. After the Llm. Llm. Puts out like text and free form, text and stuff like that. But it's not very Hai: code friendly, but if it's structure output, then it's very friendly to the next step you want to do in your in your pipeline, or whatever Meri Nova: It's it's like responses. Api and Openai doesn't have the strict mode Hai: Oh, that's like the new Api from them, right Meri Nova: Yeah. So because I had the similar issue. But I was curious. By the way, what do you think about the difference between using base model from Pydantic and just the Json format Hai: Base model Hai: you can. So pydantic is just a way to declare a schema. So the same way you would do with like a typescript interface or a zot schema Hai: The reason why some libraries allow you to use pydantic to like Hai: declare a schema like this is because it gets converted to this before it hits the model. Hai: Pydentic is just for you. It's for your developer experience. Hai: If you're a python dev, you would rather be writing hydantic classes as opposed to like writing this Hai: stupid like Json. And you know how many like, you know, you have to know when to tap, which is like the thing that I hate the most like. How many taps is right like there's like 4 or 5 right like, unless you have like Hai: es lint, or whatever, or like prettier, you wouldn't be able to tell. But if you have Hai: base model from like Pydantic, then you can just create a class, for like like this could be a class right? Parameters could be a class in Pydantic. And then in here, instead of declaring like a vomit here, you just say equals to parameter class, and that would be so much nicer than. Hai: And then the parameter class could be reused somewhere else. So like the reason why we use zod and typescript Hai: and and pydantic is because we can. Hai: The code looks nicer. You can reuse those classes in different schemas. Hai: This is basically like hard coding stuff, right? But this is what the model sees. Your pydantic pydantic class will be converted to this before it hits the model Meri Nova: I thought they got popular because they're more performant or something. I don't know why people, I guess it's just the developer experience. Then Hai: Model doesn't know what the I mean. The model knows what a Pydentic class is, but, like the Api endpoint requires you to hit it with a Json like this Meri Nova: Cool. Got it? Athira Praveen: And do you have to use tool calling to get a structured response like structured output response? Or can we do it without tool Hai: You can do tool calling with that structure output. You can do tool calling Athira Praveen: No, no, sorry the other way, like I want a structured output. But do I need to Hai: Oh, yeah, you can use structure. You can do structure output with that tool calling. So you can have like a basically like a conversation with an AI and be like you just every time I ask you something. You just kind of respond with a Json, and it would do that for you Athira Praveen: Okay. So this is, I can bake this structure into my prompt Hai: Yeah, you should. But if you hit Openai models, you send that as part of your schema. Hai: So that would be like something like Hai: Let me see if I can find an example here Sagar: I have shared an example in the chat. That's exactly. I was also going to ask you like, it's for getting structured output, I was able to use the prompt itself to Sagar: get me the desired format from the Llm. Hai: Yeah, so, usually. Athira Praveen: I am doing that right now, but I feel like there are like my, it's Athira Praveen: There are issues and how it's getting parsed into my database, and I feel like it's probably something to do with Athira Praveen: my output response number Hai: 2 places you have to do it. 1st is in the system message, and the second is in like whatever the property that the Api allows you to pass in the schema. So let me try to find if the schema for because when you hit Openai, or if you're using AI SDK like. There's a schema property that you can pass in which gets passed through to Openai, or to whatever model provider Hai: So you need to find that. Hai: Oh, do do Hai: let's see if I can find it. So it usually comes with like, the Oh, it's it's called generate object. That's what's called Hai: generate object. Hai: Yeah. Hai: Yeah. So usually. Hai: if you hit the Openai SDK directly, there's also a schema property. If you hit it through AI SDK, there's a schema property. Hai: and then you can pass your schema in here, which is usually a Json or zot schema. Hai: and then, if you want better performance, you say the whole schema again in your, in your. Hai: in your prompt. Hai: It says from here. But you can have a system message. Hai: So basically, you want to kind of like reiterate that you want this particular format. Hai: I wonder Athira Praveen: In the, in the schema property, and in the prompt right here Hai: Pretty much like just same thing with like function calling. So you just kind of it knows that there are these functions you can call. But then you also want to like tell it again. And this is very anecdotally like I do that. But then again, I don't know. I actually don't know if I take that out, it'll still work as just just as well. But I think I put that in for for this particular reason, which is like it wasn't doing well. So I had to like kind of repeat. Hai: when why use what and what format in the system message. Hai: even though I have it in these correct properties Athira Praveen: And my schema here is basically like the database schema. Right? So I have to put that in a Json format. Okay. Hai: Yeah, depends on what you're trying to do. Like trying to save it to a database. Yeah. So it's gonna be like Hai: all the fields of the columns, and you know Athira Praveen: Yep. Hai: What? What like, whether it's string or integer and stuff like that for each. So Hai: yeah, that's a good question. Athira Praveen: Okay. Sagar: Yeah, I am wondering, given that I've been using prompt template for a while. And I've been also retrieving a structured response very consistently. So I'm just wondering why different approaches. And why not? Sagar: A singular approach like which one to choose? Then I'm I'm now little puzzled right now. Hai: You're getting good performance and don't change anything Sagar: Okay. Hai: Yeah, if if you don't, then now, we're going to enter the territory of experimentation. Hai: that's the name of the game. Hai: that's why we have like prompt optimization library stuff. Hai: Because if it works like the that's amazing. Hai: Like, keep keep keep doing that. If it doesn't work, then there are a couple options you have. Hai: Yeah, I wouldn't add more stuff if if it's already working Hai: anyways. You guys want to stick around, I'll maybe I'll set up Hai: some tool calling and structured output after the call. Hai: And when I say after the call, I mean like, now, if you want to stick around, stick around for that, if not Hai: feel free to leave. Hai: Yeah, how's that sound? Hai: Okay, cool. Actually, before we before we end the lecture. Hai: let's go back to figma I want. I want you guys to populate the Hai: what's the tool calling and structure output table from what you learned today. Hai: And if you know more than your classmates feel free to put stuff in there, because Hai: everyone's trying to learn so Meri Nova: This is so cool that we can write in the same environment. I just love this Hai: Is this her 1st time doing remote work, Mary? Hai: I really hate this stuff. By the way, like when I pull up slack or like figma. I get anxiety Meri Nova: Why? Hai: Brings back memories. Hai: That guardrail piece for a tool calling is such a hidden gem. Hai: Well, while you guys are doing that, keep doing that Hai: I'm gonna pull up the code for for stuff. Hai: Oh, this is my hackathon project. So funny Meri Nova: Are you gonna open? Source? It? Hai: I might, I might. Hai: I'm trying to think what to do with it Hai: like. So a bunch of Vcs like reached out after my presentation. But after my pitch, and they were like, Oh, I need this stuff like 2 weeks ago, me and the team was trying to like copy some features from like a very popular open source project. So I'm like. Hai: let's chat. Let's chat. Hai: And like, was, it was difficult, because this is like a this is like a developer tool, right? So half of the half the Vcs in the room were like non-technical. So I had to really dumb it down. Hai: So I just say, like, Oh, you know, when your team is like team of developers is like trying to like replicate a feature from somewhere else. And Hai: you can reduce that time from like 7 days to like one day, because this will just tell you what to replicate. Hai: And then half of them were like somewhat technical. So that was easier. Hai: We'll put the circle right here. Hai: But yeah, oh, did I? Did I delete somebody's circle? Was it supposed to be there Hai: if it wasn't my apologies. Hai: anyways, I'm gonna pull up the but the Ekta: I had. I had a question Hai: Yep. Ekta: Just thinking about a use case in my project, and maybe I can reiterate that, and you can correct me if I'm thinking the right way. So let's say, I have multiple sources to call or like cross referencing multiple Apis so that I'm using and Ekta: to calling. Calling. I want this Hai: Yes. Ekta: Application to be called for certain topic, and not for, like something like that. Hai: And the team at Weev. 8 calls it agentic rag. So you basically, you wrap the source of knowledge inside of a tool. And then you tell it when to call the source of knowledge Ekta: So it's a it's a known pattern, so Ekta: like I can give it a Ekta: structured query with Json like abstract title, Hai: You just gotta tell it the structure it is for your arguments or your parameters. Hai: Where's the getting started? Repo? Hai: Try and find it? Hai: Is it this one? No. Meri Nova: What was there getting started? The couch task, the vibe Hai: Like the the very 1st repo that I made like in the 1st lecture, where I set up like light. Lm, and AI SDK Meri Nova: On your own workshop repo actually Hai: Oh, Meri Nova: Their booking. Hai: Here. Hai: Okay? Hai: And if I have it on my computer Hai: and then AI workshop. Yay, okay. So we got this. I'm gonna Bootcam. AI SDK, no Bootcam, I think Hai: he Hai: can start. Hai: Boom. Hai: Okay, no, it's not it. Hai: Where am I? Where am I right now? Hai: Well, my word, now. Oh, I have 2 files. Okay. Hai: Now, now, it starts to come back to me. Hai: Okay, we pulled out this starter code and then we're gonna do some stuff. Hai: I'm glad I had this. So I don't have to like set up everything again. Hai: Okay, zoom in real quick. Hai: Okay, so this is like, very quick. AI SDK like, write a short poem. Hai: So instead of doing this, what we're gonna do is we're gonna set up a function to Hai: wonder if I should do tool calling or structure output. So tool calling is basically structure output. It's a structured output of the name of the function. And then the arguments that go into the function. Hai: So let's do structure output. First, st because that predates tool calling. Hai: So let me pull up the documentation for AI SDK right here. Hai: So when you're using the AI SDK. There are many ways to generate stuff. Hai: When you want to generate text, you call the generate text method from the AI SDK library like this Hai: when you want to. When you want to do just structured outputs, you, do you use generate object. Hai: So we're, this is what we're gonna be using. Hai: So I'm going to do this. Go back here. Hai: We got generate text. But okay, cool. Hai: But what I want to do is I want to create another file here. Perhaps I call this Hai: structured output example, dot js, because this is a Javascript repo. For whatever reason. Hai: my brother in Christ, you made the repo. Hai: I made the repo. Hai: Okay, so Hai: to do structured outputs, you can do like the regular Json stuff, which is very ugly, right? But you can in Nextjs or in typescript or Javascript. You can use this library called Zod. Hai: Does anyone here is using the zaw right now or no. Okay. I see one hand. Hai: Duane, you're just checking your head. Will make your life easier, and cursor is also very familiar with zod, so Hai: this is how you would create a schema in that. Hai: It's very clean. If you were to do this in like straight straight Json, then it's going to be ballooned, and then you can't reuse some of the schemas in different schemas Hai: so uses. Aw, that's what I'm trying to say. It's recommended by Openai. All that stuff. Hai: So we're going to be doing this. So I guess we're going to import zod as well. Hai: And then we're just gonna say Hai: I guess I need a schema first.st Hai: What am I trying to do? So my input, my input would be something like, Hai: a website or something or website. Hai: HTML, Hai: see? It works with HTML. And like Markdown, and all that stuff, too. If you're passing HTML, and you're trying to get like structured output, it usually works with a strong enough model Hai: and then the output I wanted to be. I wanted to find Hai: I want to figure out what website I want to extract first, st Hai: do you guys have a website that you wanna like, extract information from Oren: Hey? I'm trying to extract a to scrape a lot of recipes, so maybe fuse.com, or something Hai: You call it serious? Eat Oren: Yeah. Hai: So we're gonna pick Hai: I like how they don't just give me like a recipe. Hai: None of this is recipes Hai: recipe. Oh, there you go, recipe. Oh. Hai: there are so many things to choose. Hai: This one looks nice. Hai: Okay, let's say we want this. And the quick and dirty way is that I want to. Let's just pretend we have a tool to script the web Hai: we don't have it built in, so I'm just going to go to file, crawl, and which is a web scraping tool. And I'm just going to paste the thing in. Hai: And then what I'm going to do is I'm going to get the markdown content back. Or I think I promised you guys. HTML, so I'm actually going to do that Hai: include HTML content. There you go run again. Hai: So I'm going to hit that website. It's going to bring back both the Markdown and the HTML. So this HTML, it's incredibly dense, incredibly long. Hai: But with with a good enough model we can definitely get the response back. Hai: I'm gonna copy that. And then I'm gonna make new file here. And I'm gonna say, recipe Hai: data or something like that recipe recipe. Hai: Okay, so Hai: and I'm going to say this. And then I'm going to give it a basically a string. I want it to be Hai: so. I use the string literal so that I can kind of do this. Why, if you use quotes, it will not do that for you. Hai: This is basically the website. So I'm going to save it in this, in here. It's called recipe. Hai: Bring it back in here. So the input would be Hai: this, basically. So I'll say, import recipe from I'm gonna say. Hai: dot slash recipe. Thank you very much. Hai: And then the output of this. What I want it to look like is I want Hai: probably. Let me scroll down to the steps. Hai: maybe ingredients, and then directions, directions could be a list of string. Each step is a string. Hai: And then, okay, you know what I'm gonna this is the part where you use Hai: cursor because I don't want to type all this stuff. Hai: So I'm gonna pull up cursor and be like Hai: yo make me a zot schema for recipe. Hai: I need the following fields required, and then I need ingredients list of string. Hai: I need a steps list of string. Hai: and I need maybe like time, and then I need Hai: Time would just be like seconds. Hai: and this would be a number. Hai: and then I need maybe something like Hai: Where else what else would would I care about when I make this? Hai: It's historical significance. Hai: Why like, why? Why was someone? Why was why should someone make this? Hai: And because we're dealing with AI, we're just gonna let it know so that it can kind of bake that in Oren: Maybe we can make like an effort level Hai: No. Oren: So difficulty, or something Hai: Okay, I like that. It's actually I don't know if it's in here, which is a good kind of example to see if okay it does. Maybe it couldn't find anything like what is it going to do? And we can maybe Oren: Like the amount of ingredients or the amount of time. Oren: Some sort of a combination, maybe Hai: Yeah. Hai: But I do want to see this, because I want to see like, okay, so how is it going to deal with not being able to find information from the text. Hai: Okay. So we got the zot schema for recipe here, and then Hai: we have to delete this, because if if it's the field is required and it doesn't pass anything in. That's when our code breaks. Hai: That's when. Actually, no, that's not when our code breaks. That's when the the AI provider will send back an error and be like yo. We couldn't generate anything because there was one field that was required, but we couldn't do anything about it. Hai: So sen. Hai: So just a quick look at my setup. I got 3.7 sonnet Sagar: While it's working a quick question. Hi, for your hackathon. How did you manage to afford the tokenization cost? For while reading huge port bases. Sagar: I mean, you like even reading one project. It would have costed you so did you made any optimizations while Sagar: reading those code bases or Hai: Yeah, I use this tool called repo Repo Max. And there's a Hai: there's a flag that you can enable where you can compress the Hai: the structure of the code without it losing its its like relationship. For some reason. Hai: I think, for example, you Hai: what it does, it basically treats certain functions unless they are significant and used a lot. It treats certain functions as like an abstract function. So it takes. Basically it keeps the Hai: it keeps the function name and the parameters and the return output, and just leave everything in the middle blank. Hai: so like you almost like treat it as like. No, there's no implementation, but you kind of know when to use it. Hai: So there's compressed mode. And also I, somebody. A lot of people use this. I burn like $17 like overnight. Sagar: So. Hai: I don't think I I don't think Sagar: But which tool? Sorry to interrupt you. Which tool did you mention? The name Hai: It's called Rebo Max. Hai: Let me pull, mix my bad. Hai: Okay, you can use this locally to like, just run it against your code base. And now you get like a markdown on pretty much all the stuff in your code. Base Sagar: Thank you. Hai: Yep, anyways, going back to this, we got the recipe schema. Hai: which looks fine. We're not. We're not here yet, delete this except Hai: recipe schema. Okay? So we're going to go back to our documentation here. Hai: So what I'm seeing here is I'm seeing a constant result here. And then this is an asynchronous function. So we have to await it. Hai: So I'm just going to copy this right here. Hai: Go down here. And I'm just going to say, Okay, so I need to pass something into this generate object function. Hai: So what goes in here? What goes in here? Okay, we've got to pass in the model. So 1st go passing the model Hai: model, I'm gonna say, Gpt, 4. 0, spurn some money. Hai: And then what do you want? Oh. Hai: it's supposed to be an object, my guys, there you go Hai: model. And then I think next thing is default, object, generation mode. Json. Hai: Okay, so since this is open, AI, and I just told you guys about strict mode. I wonder if I can pass in strict in here? Hai: Yes, I can can't wait. Hai: Let's see. Hai: Let me actually look that up. I don't know if that's true. I don't know if my linter is working well, so Hai: let me just double check and see if strict mode goes in there, or should I look up. Hai: generate object here because we were in the documentation for testing, which is different? Hai: Okay, schema goes into this. So Schema goes into the Schema property. Hai: And I hope you guys watch me do this. Hai: Because I'm like looking at documentation. I don't have know these things off top of my head right? So you can. You can totally do what I'm just doing right now Hai: and then Hai: you'll get there if you feel like, oh, there's a lot of stuff like, look at me. I'm just literally just reading the docs and trying one step at a time. Hai: So, Schema, we're passing the recipe schema over here, which we've declared with Zod. Hai: if you want to write zart by hand, use AI. It's pretty straightforward. Hai: And then the prompt goes down here. So I wonder if I can find the strict mode in here. So. Hai: as you see, whenever I have a question about something, I'm usually just looking in through the documentation and see if I can find it. Hai: So I type in strict. And down here I see structure output from Openai. Hai: Structured output. True. Okay, okay, okay. Hai: To do. Pdf support predicted outputs. Hai: Okay? So all it's saying here is that it says, as long as I have the structure outputs to true. Here Hai: I'll be fine. Hai: Which is not true. Hai: and I'm going to cheat a little bit because I looked at this during my hackathon. Hai: There's a property that I can pass in to get truly truly strict mode. Hai: So here let me pull up my hackathon project real quick, strict. Hai: There we go. Hai: So this also uses the AI SDK and Hai: I have to declare a strict mode in the open. AI's model client, basically. Hai: So I can't. I can't do that in the generate. Let me pull up the other code again. I can't do it in the generate object method from AI SDK. I have to do it Hai: earlier in that. Hai: So I have to do it when I when I create the open AI like model object. Hai: So let me scroll up here real quick. Create open AI from AI SDK slash openai. Okay? So I need that Hai: gonna pass that in here. Hai: I don't know if I install this, though. Let's see. Hai: there's no way for me to know that's great. Hai: And then Hai: let's see where I use this. Go down to where I use it, and then I have to do this. Hai: So there's a property called compatibility. Hai: And then I had to pass in the key. Strict here. The value strict here to this key. Hai: So that's how you get strict mode in Openai when you're using Openai, I mean using AI SDK Hai: very hard to find. But now, you know. Hai: because I did find this when I was doing it. Okay, so going back to this, now, we're we're pretty much good to go. Hai: I don't know if we can still need to pass instructor output. True, did I do it here? Hai: I don't think so, and it worked fine, so Athira Praveen: Don't we need the prompt Hai: Yes, we need to prompt. I'm trying to see if I need to like, you know. Do like the thing that the documentation says. It says, like structure. Output's true. Hai: So it seems like I don't have to Hai: when I have my strict mode enabled. Hai: Yeah. So the prompt, the prompt is Hai: well. The AI has already written the prompt for me, which is great. Hai: Extract the following, let's extract the recipe information from the provider. HTML. Hai: Return a structure in Json, with Hai: looks great looks great, and the HTML here does not exist. So we're going to have to like figure out how to pass that in. But we already have that right here. Let's go recipe. Hai: So we got recipe here beside here. Hai: Okay? Why, the heck, is it doing this? Okay, okay. Hai: I don't know why it's like format like this. That's funny. But this is basically our our website. So it's good. Hai: We haven't all right. So that being said. Hai: we have the schema passed in. We have a prompt here that tells it to extract some stuff, and Hai: we've got strict mode on, and I'll do one with that strict mode on so that you guys can see what it looks like. Hai: And yeah, thank you. AI, Hai: that's pretty much it. Let's find it. Hai: Go to my terminal. Hai: And then this is in Boot Camp AI SDK, so I'm just gonna see the in boot camp. AI SDK, and I think I just have to do a node and then structured output. I think last time I ran this I need to add some stuff in the beginning, so let me see if I need that. Hai: So here we have the Hai: Actually, I don't need to do anything Hai: that's great, but I do need. I do need to do this. I remember from last time, because Hai: because we're running this script on its own, it needs to load like the Hai: the virtual and the environment variables which contains the open Api key. Hai: If we don't do this, we didn't. We didn't have it last time, so I think we'd struggle with that for a little bit. Hai: Other than that, I think we're good to go. Hai: Yeah, save. And then I'm going to run this. And something's gonna happen. Obviously. Hai: it says, cannot find package. AI imported from blah. Blah, blah blah. Hai: Well, why is that? Oh, because it's a new computer. I didn't Pnpm install. Hai: Psych Hai: it's almost like there's always some stuff that is just in the way, right? Always. It's the nature of the job. Hai: And then I do this for 4HA day, and I get pissed by the end. Hai: I need to go like blow some steam. Hai: Okay, so Pnpmi boom done. Now we got the note modules. Now we know we've installed some stuff. Here. Hai: run this again, please. Of course something else happens. Hai: We need. Zod, we don't have. Zod, it's it's 9 30. Just Hai: AI. I don't know. Like, I don't do. Hai: Yeah. One day we will have this bigger app for us. Oh, we have an error. Let me just do that for you real quick. Hai: I'll fix that for you. Hai: What else? What else do we need? Hai: Oh, yeah, okay. So because we're running this as a script we need. And we didn't specify in our packagejson. This is supposed to be a module. I believe that's why we need to basically Hai: do this. And if you're using next nextjs, you don't have to worry about any of this stuff, because when you instantiate a project in next, everything is just taken care of for you. Typescript types, everything. Typescript config. Hai: all that stuff easy. Hai: What else do we need to do here? I will not touch this code base ever again. I'm going to pull up like Nextjs next time, because this is just wasting my time. Hai: But what else require is not defined es module scope you can use. Hai: I was trying to accommodate. Hai: I was trying to accommodate import. Thank you. There you go. Hai: Do I need this? Hai: Maybe I do run. Okay, something else happened. Hai: Model does not have a default. Object generation mode. Hai: Okay? What do you mean by that? Oh, we have to pass this in. Hai: It's dim. Hai: There you go. Hai: I believe that's that's how it's supposed to work. There we go. Yeah, perfect. Hai: Ron. Okay, what else got on? Swear to God. We module type. Hai: It's not specified. And it doesn't parse as common. Js, which file is this Hai: structured output example? Okay, reparsing as es module, because Hai: model does not have a default object. No, wait! Hai: This is a old hold on. Hai: Hmm! Hai: This happens again. What? The heck? Hai: I thought I passed something in open the eye right here. What are you talking about? Hai: Oh, I need to. I need to give it a model name, I think. Hai: Okay, let me pull up my hackathon code again. Where do I declare it? Oh, there we go! Hai: That's different. So Openai object goes into provider options. Okay, okay. Hai: no, actually, that's not true. Let me backtrack that real quick. Hai: This goes where this goes where Hai: model model is supposed supposed to be. Oh, the model is when you use this to wrap the name. Hai: My God, okay. So I need a model right here, and the model is for Gpt. For linking Hai: and then pass the model down here. I can't just pass the open AI object down there, all right. Hai: What else? Hai: Openai Api key is missing. Hai: Bro, that's why I have the freaking thing here. Hai: Okay. Hai: And so I knew index. Dot. Js worked. I knew that from last time. Hai: So what did I do in here? That was different like other than this. Hai: can I just have this in here? I think so. Hai: I think so. Hai: But it's like, No, what's happening here, Chad, does anyone know? Hai: Okay, this doesn't parse as a common Js module. Hai: Well, then, why does this parse as a common Js module? I didn't do anything different. Hai: let me think for a sec. Hai: Oh, I think I think I know why Hai: we all use require here, but in here we use like regular Hai: stuff, which is not what we're supposed to do. I don't got time to change all this. I'm just gonna say, like Hai: use. Require here for me, please. Hai: Thank you very much. Looks about right? Hai: It's like ticking back 20 years. Hai: Oh, it requires other. Hai: What do you mean? Hai: I thought, this is what you wanted. Okay. Hai: then go in here and see this real quick. Hai: honestly, honestly, I'm gonna pass this to AI. Don't want to deal with this right now. Hai: So if you're on Nextjs, you don't have to think about what I'm doing right now, because this is just a script running on its own. Hai: So now we're we're gonna let AI deal with this stuff Meri Nova: So what was the problem? Hai: So the problem is, this is running as a script in node, and I don't remember if I declared the configuration for it to be an es module or not. Hai: and I'm too lazy to find out right now, because it's 9 30. So Christmas Hai: is going to help me. Hai: Okay, also, if it's bringing back the import statements. Hai: I don't know if that's how it's supposed to work. Hai: But then, when we add the type module to the package. Sdsm, the other file is probably not going to work. Hai: but we can try that. Hai: So we have web. Add type module to package a Json. Hai: Now it's it's treated as an es module, so that can do like the modern type Javascript stuff Hai: instead of this, like backwards Meri Nova: Where do I hide? Do you give like context more than that file? Can you give, like the entire folder. Meri Nova: the context of the agent Hai: Yep, you can drag a a full folder in. Hai: Okay, no need to run this. Hai: I will just. I will just accept whatever you just wrote. Hai: So what it changed was the type module which is the configuration silliness Hai: that has consequences. But whatever we'll accept that. Hai: and then here we just switch it back to import statements here, so that because now this is treated as an es module. Hai: so let me run this again. Hai: Swear to God. Hai: What, what? Hai: Oh, do I not? Oh, this is a new computer. What am I keep thinking? I don't have a dot envy file in here. Chat. Hai: My brain is so cooked right now. Okay, dot envy. Hai: it's like the error message is accurate, and I'm like, no, you don't know what you're talking about. I don't know what I'm talking about. Hai: Hey? Hai: Mary, are you seeing me having a a mental breakdown online? Meri Nova: Yeah, I'm actually laughing behind the mute Hai: Oh, it's still on. Okay. Cool. Hai: Get me my Api key real quick. Hai: Api key starter boot camp boot camp 2. Hai: Create secret key copy. Hai: Go down here. Hai: The Api key equals to this string. Right here. Close, run it again. Hai: Yay, no, God! Hai: What? What? What is it this time. Hai: Oh, okay, I see this model. Maximum context is 28,000 tokens. Hai: a hundred 37,000 tokens. Hai: okay, so we gotta find another recipe my guy. It's too much Oren: Let's do. Let's see it. I'll give you something Hai: Actually, before we do that, let me. I have second thoughts. I have second thoughts here. Hai: Okay, now we can. Now we can continue. Hai: That's a lot of tokens. Bro, okay, Hai: I'm down to do another recipe. I'm down to like copy less of this Meri Nova: That's a lot Hai: Yeah, because again. Oren: Blocked. Yeah. Hai: A lot of this is just like the Svg icons and stuff Oren: Yeah. I don't know why Hai: Well, that's the raw HTML. But if you want like clean Markdown, you can do this, which is like, you know, pretty easy for a model. Hai: I guess I was trying to like. See if the model can like, just look at straight HTML, and like do it. Hai: I can just copy of half of it, I don't know which half Meri Nova: I think different. Page Hai: They're all going to have these like Svgs. We're not going to be able to Meri Nova: Let's see. Hai: But you know, we can just choose, because 128,000 right? And it was a hundred 37,000 tokens, so if I choose half, if I take up half of it, there should be plenty of room Oren: Yeah, you can cut all the sheet. And yeah. Hai: Yeah. Let me use this. Meri Nova: What kind of recipe is that, my gosh Hai: Creep. Hai: Okay. Oren: Something very technical. Hai: Something very technical. Hai: Boom recipe in here. Boom, save Oren: Good for machines. Hai: Alright, we're running this again. Hai: Boom. Now, we're talking now it's loading. Hai: all right start start betting go on polymarket is this going to error out again. Hai: I doubt it. Hai: Oh, my God! Meri Nova: Let's go. Hai: My God! Hai: What happened? Meri Nova: And ran, look at that Hai: Oh, right it was. It was a good generation. We just didn't parse it in our response. Hai: That's why it gave us like a whole bunch of stuff here. So Hai: the the key is is called object. Hai: I'm just gonna add here, object. And then we got ingredients. We got steps. We got Hai: time and effort. Easy. Not sure where they got it from. Hai: Does it say? Easy in here anywhere? Hai: No, no, no, regardless of which. Oh, okay, I guess. Hai: like, I said, it is easy. Hai: The information is in here. Hai: Cool. Okay, let me parse this real quick. And then I'm just gonna Hai: do that. So one last thing, because that was a long response. Hai: So when you do structure outputs in AI SDK, it comes to in the object key. Hai: And then now you can save this into a dB like pretty easily. Hai: I wish I had more time to do like 2 calling, but I'm getting pretty pretty tired right now. Hai: But yeah, structure output. And then you can parse this array save each one into the dB stuff like that. Hai: Yep, I'm gonna push this out. So if you guys need like like Hai: beep beep, beep Meri Nova: Homework be for this week using tool calling Meri Nova: and structured outputs in your apps. Hai: I I feel we should do our homework as like. Hai: I wonder if we should even do homeworks, because, like you guys are all building your own stuff. Some of you guys are already using this. If you haven't used this, and this gives you an idea to build something, then, yeah, go ahead and do it. Hai: Are you already on track to use tools and structure outputs? Meri Nova: I guess Hai: If you have questions, now's the time to ask. Meri Nova: Yeah, it's always about the progress in your project, anyway. So it doesn't really have to be about the topic Hai: I feel like every every week we do this. It's kind of a way to like kind of inspire you to think about. Use cases to build as opposed to. Oh, you have to do this Hai: Because I kind of wanted you guys to think as like Hai: like, you're building a product instead of like Hai: me telling you what to build, because, like your users, are not going to tell you what to build. They'll tell you what they think you should build, which you, always taking. Take it with a grain of salt. You should have some sort of opinion about what you're trying to build. Hai: have tastes around Hai: what's important, and then deliver it because people will be asking for horses right if we we ask them. Hai: But they didn't want to make cars. Hai: Structured outputs. Hai: I think that French domain. Yeah, there we go. So it should be on here now. Hai: Boom! Hai: Yep, that's it for tonight. We're like what? Hai: 4 weeks away, including this week Meri Nova: 3 weeks. Hai: Oh, I thought, it's like a 6 weeks. Okay, we're 3 weeks away. So you guys gotta get on it. You know, if you need anything. Let me know. Let Mary know Hai: we're here for you guys. But I want you guys to cross the finish line. I want you guys to like, deploy your projects. Hai: feel super proud about it. Hai: Type shit. Well, I mean, this is like recorded. So Hai: probably should have said that. But I said that a lot during the hackathon Meri Nova: That's what you should be talking about, actually, and walking us through your project. That would have been more fun for me, at least. Hai: Oh, I mean, if you're sticking around, let me do that right real quick, quick, 30 seconds overview. So Hai: what this does, is it? Yeah, let me pull it up. Pnpm, dev, Hai: I think it should be running right now. 3,000, perhaps. Yeah. So this is what it looks like. You can drop in like a Github URL of any project. Hai: and it should be able to tell you how to rebuild it feature by feature. Hai: So let's say we have the. We have this, for example. Hai: I don't know if I can Meri Nova: I don't know if you can Hai: You messed it. Meri Nova: Short, it's not fun. Hai: What? Hai: Oh, it can't do, nested both. So let's see if I do. My github trying to find something Hai: kind of like Meri Nova: Dolanger. Hai: Yeah, I must go right here. Hai: Go here, drop the URL in explore. Hai: So it pulls in basically all the code from that particular repo. Hai: and then, you know, you can check, you see. Oh, this is the the actual repo. I can read the code from here, but that's not the special part. The special part is in this tab right here. It says, features walkthroughs. Hai: But I want to show you this first.st So there's this thing called Lms dot txt, which means that Hai: this is basically the entire code base that I just pasted in. But in one long string, so that this can be used. When I go to cursor. I can paste this whole thing in and start asking questions. Hai: That's cool. But this right here is what I spend most of my time on. Hai: It's called features walkthroughs. So what it's doing right now, is it? Hai: Basically looked at this. And then it ran, Gemini, 2.0 flash to kind of like, look up all the features that is available in the code base. Hai: And let me show you that real quick. Hai: So I use a server server function, server action Hai: for this. I think it's called generic features right here. Yep. Hai: and then here, what I do is I ran the but but Hai: I ran the AI SDK generate object function that we just saw earlier. Hai: And then I pass in a schema, and then I have a system message, and the prompt that says, analyze this code base. And here's what the schema looks like. It's also zod, and it's basically an array. So it's an object first, st and then in that object there's an array Hai: of a string, and then I have to give a description. Hai: so you can give a description inside of a zot schema, so that your AI has an easier time generating the structure output for you. Hai: because maybe, when I say list of features, I need to clarify. What do I mean by that? Right? Hai: That's why I have a describe key here. Hai: pretty straightforward, right like it's just a list of strings. And as you can see these are the strings. Hai: This is shrink. So map scraping, using playwright. HTML, tag, extraction. These are all the features that were included in the repo. Hai: So as somebody who's completely new to this repo, it's easier for you to look at this and be like, Oh, it can do this. It can do that. I can learn that as opposed to like all code. Then you got to go and dig in like, see? Like what what's doing, what? Hai: So that's the generate features server action. Hai: And then, yeah. And then here, what I can do is I can click on any of these, or I can search for ones, I can say, like scrape or something. Hai: But I maybe I want to do this. I want to click on this. Now it's going to run another server action to Hai: generate a walk a walkthrough. So this is where I use gemini 2.0 Hai: pro, I think, and then it was breaking all the time, because sometimes the code base is very long. Hai: The contact is very long. So Hai: I have a fallback to Openai, O 3 mini Hai: and this is like this huge system message to be like, Hey, like this is the code base you're supposed to generate. Blah blah. Hai: And then that's the system message, and then very important, you know, preserve all white space indentations. So that that looks nice. Hai: And then I just like stuff in the code-based content. And then the feature to implement, which is literally just the text that we just saw from the list Hai: so that it knows what we're trying to do. Hai: So the model is for flashlight, and then I pass in the schema. This is the Walkthrough schema. Hai: which has the file name, the full code of the file, which is like, if it's looking at a file, it should return the full code. Hai: The reason why I have this is because this is kind of a hack. I want to compare the current code to the full code of the file and then highlight it. Hai: What I should have done was parse this from like the initial repo pool and then not make the model do this again because it's just such a waste of tokens. Hai: And then the current code is like, Okay, which code are we talking about at this step? Hai: And then an explanation of the step? Hai: So it's almost kind of like a slideshow, in a sense. So now it's done. I just had to follow along with this. So first, st you know, you create a new file called Scriptpy, and then these are the link. I mean the code for that file. And then you click. Once you're done with that step you click on next. Hai: Now you're going to add these to the script to your script that py Hai: file, and it highlights the stuff that is new. And then there's this like new function down here, and then it tells you like, why we're doing it. Hai: And then last minute I had like a audio thing, too. Hai: so like, let's add a function. Hai: It was like, Tell you like, kind of got you through it. Hai: And then, if I leave it on, it'll actually go to the next stage automatically Hai: and then see that like, it'll just scroll to the part that you need to do, and it'll tell you like, why you're doing it, so that as you're coding along. You kind of like know the reason why you're doing certain things. Hai: and then you can keep doing that. And then by the end you would have a working whatever that you're trying to learn. Hai: So 9 steps to set up playwright. Hai: And this is pulled from the code base. So it's Hai: you're trying to learn a code base. Then you know, you can do that. Hai: And then this is the code to generate the the Walkthrough, and then the fallback is obviously Openai, because Gemini was kind of like finicky. Hai: So I have this strict mode here. I use, O 3 mini, which is a decent model, and it has a Hai: special property called reasoning effort. Hai: If I turn this to high, it'll take like forever to load. So I had a little bit low. Hai: and then this is repo mix. Hai: This is the tricky part, because repo mix is actually a cli tool. So what I had to do was, I had to like Hai: finesse it, so it run in the vercel like serverless function, which is kind of tricky. Hai: This is probably this part I spend the most time on. To be honest. Meri Nova: Wait. Meri Nova: Can I ask a question? Hai: Yeah. Meri Nova: So on the generated walkthrough. I'm just curious how you're generating the ui like that Hai: Oh, the ui, that's the ui stuff. So I have Hai: See Meri Nova: Because, like it generates after you generate the steps, and then it just creates the ui components around each step. No Hai: This ui is pre-built. So Hai: I made it so that once the data for the Walkthrough is available, then it will display this kind of like Walkthrough, editor like this Meri Nova: Yeah, but like, on the left there's a number of steps right? So do you define, like, how many number of like slides you need to do for the ui Hai: Oh, no, it will just take, however many it wants to generate. Hai: So next, you know, if we go back here, we go to another one Hai: could have been like 5 steps like I don't have like a specified number of steps, for it Meri Nova: Yeah, that's what I'm asking, how do you get that into the ui? Because Meri Nova: I know you can generate like strings right for the output. But what about ui components? Hai: So this comes back like I said, like, it's, this is a list of stuff. Hai: Yeah, let me show you the walkthrough. So this is an object. Hai: Every every walkthrough step is an object that has a file, name. Hai: the all the code, some of the code to highlight, and then an explanation, and then I Hai: put that into the actual schema here, where it has, like a 1, property called steps, and the Steps property would just be a list of all these things. Hai: basically, and then pass that to the generate object server action here. Hai: where this says Schema, something that knows that. Oh. Hai: when I input the your, the repo as text. I want structure, output out. Hai: as I have declared in the other file. Hai: So this is where I get this. The list of steps. And then once I get the list of steps, I'm going to display that in the Walkthrough editor here which is just react stuff. Hai: Jesus. Hai: He has a lot of stuff just to get like the editor working as well. Hai: yeah. So once the steps are available, then we're just gonna like Hai: do a bunch of stuff here. Hai: I don't remember what I wrote here. Hai: Oh, yeah, it's not like 2h Meri Nova: Okay, that's like a lot. Yeah, it's a lot of Meri Nova: front end code. Apparently. Okay, cool Hai: Yeah. So also a lot of vibe coding, too, as you can tell from, like the freaking comments that I never wrote here. Hai: But essentially, there's like a component here, and there's a component here. Hai: and then there's a component here. And then, once the steps are available, I'm just going to pass the data down to like this separately, this separately, this separately, because every step has, like complete context of like which step it is in the array the content of the file, and then the explanation. So Meri Nova: Like in the metadata, or something Meri Nova: Cool. Hai: Ye. Meri Nova: Right. Hai: What are the highlights for? Hai: Say that one more time Gil: The highlights. What what are they for Hai: The what sorry Gil: The highlights on yellow Hai: Oh. Hai: the highlights are for you to know that, hey, this lot, this file is pretty long. But this is the 2 lines that we gonna be paying attention to. Hai: So it's supposed to be, for like one step at a time and Hai: the highlighted step, the highlighted code is the code that is just supposed to add in a step. But then it's really hard to prompt it to like. Just write that, because, as you can see, like Hai: from here to here. It's supposed to highlight this honestly, but then, like it's kind of like screwed up there. Hai: But if you don't quick, then people won't see it Gil: Awesome. Thank you. Oren: Yeah. It's a very cool project Hai: Thank you. Hai: I'm gonna build this out this week and then put a stripe link on it Gil: Yep, make money, make it rain. Meri Nova: Are you gonna sell it to us now? Hi, I thought, we're building internal tools Hai: This every time I hit generate this shit cost me money, so Meri Nova: Oh, that's right, that's right. Hai: Pay for my time. My God. Hai: it's gonna be like $2 a month or something Meri Nova: I mean, heck people gonna pay 20 bucks a month if you want. It's a great educational project, but this is relevant for a few of the people, too, who are working on the Github or another learning platform stuff like that. Meri Nova: Think, Melissa, you were building something like this Hai: I built this for Gil. Do you know that Meri Nova: Why, Gil! Gil: You're driving. Hai: About this Gil: Because I'm the dumbest student. I need more help. Come on. Hai: He asked me about it. He's like, I wish there's a tool that could explain to me Hai: like just the stuff that I need from a code base that I found Gil: I owe you, man any design you need. I'm your man. Actually, I think this would be a really really cool, startup product. Gil: going to help a lot of people. Gil: I'll drop a few ideas to you that that I got while I was using it. Maybe they are help helpful. But this is a really good tool. Gil: Appreciate it Hai: Please send them my way. Hai: I have, every intention to productionize this, because Hai: this honestly, when I like launched it had more market pull than like cats with bats. I'm like, man. Hai: should I just do this Meri Nova: What do you mean by Market Poll, like? How many people came up to talk to you about this, or what Hai: Yeah, people will like, send me a link asap Hai: need this need this stuff today. Meri Nova: You know why, cause like I I told you today, too, there's gonna be more software engineers, thanks to AI now. And they're not gonna be like proper, I guess software engineers. But like the new generation of Hai: Talking about you guys. By the way. Meri Nova: Why are you outing me like that? No, no. But okay, yeah. Why not for vibe coders, basically. So more tools for vibe coders? Meri Nova: Actually, yep. Hai: This is basically a vibe coding tool. It only exists in the Vibe coding era. It cannot exist anywhere. Any other time in history. Meri Nova: Yeah can't. Oh, my gosh! Meri Nova: There's so much to build, all right. Hai: So before we leave quick question for you guys, how far are you guys in your project like, how do you feel? Do you feel like you're stuck or doing. Okay. Melissa: I feel good about my project. I'm in the AI implementation part. Melissa: And Melissa: yeah, it's a lot like I drop in one file and test that file, and I think it's all good. Then once I add multiple files, it's like debugging hell. But Melissa: you know, I'm up to the challenges. It's it's pretty cool, seeing it come together. Melissa: But it's it's it's it's a big task Hai: Real. Melissa: Yeah. Hai: You're crushing it. By the way, keep going Melissa: Thanks, thanks. Athira Praveen: Yeah, I was focusing on the AI part and the Ui part up until now and then. Last week I just started on the dB, side of things and the auth and everything. I'm still Athira Praveen: figuring out the kinks there. Athira Praveen: currently having some issues with how the data is getting parsed into dB. But I think the structured response that we covered today might help Athira Praveen: with some of that Hai: Let me know how it goes Athira Praveen: Thank you. Oren: Very much the same that you had Gil: That's awesome. Oren: Go on! Gil: No! No! Go ahead. All right. Oren: Okay, Oren: I was pretty stuck with a trying to get some structured output. So recipes basically and trying to vectorize it. Eventually. Oren: I I think it's gonna be a huge help like to actually get it out structured. And Oren: like also filter out a lot of scraping bullshit that I don't really need, but also getting Oren: the best of it, like just the stuff I need. Oren: And I think I'm just gonna push for all day this week. Hai: Nice? Hai: Are you using 5 crawl? Or what are you using Oren: Not really fireball. But I was trying a lot of basic stuff. But I think I'm just gonna go on to beautiful, beautiful fireball. Oren: I find a way just Hai: No problem. Oren: What is it? Hai: I strongly recommend firecrawl, firecrawl with Markdown. The reason why they have Markdown is because of the use cases like what we saw with structured output and extraction Oren: Just to make sense of it and getting all the stuff I need. I have like 10 different categories. I could vectorize Oren: And then other use cases they want to do Hai: Nice Oren: Gail, it's your turn. Gil: Cool, cool. So yeah, I'm I'm just trying to keep the scope really small. I was at the beginning. I was. Gil: yeah, throwing, like many files into the data intake process for rag. Gil: And yeah, kind of like what Melissa was saying. I was just spending a lot of time troubleshooting Gil: the multiple multiple files. So I'm just making a fat single file and using that later Hai: Yep. Gil: I can improve it, you know. And then I'm trying to figure out how to make the Gil: the output. That exactly the stuff that you were talking about today, how to get the model to provide that reliable Json output? Gil: Because if anything changes Gil: on the the Dsl doesn't work anymore, so everything that that you were saying today is is going to be really, really useful for me. Gil: And that's where I am. I'm still on the notebook. Gil: but this is like the the heavy lifting. Once I'm done with this, it's going to be easier to port it into, you know, a a python application, or Gil: Netj, or something like that. Gil: I'm still clueless about the I'm a designer, and I'm clueless about the Ui Gil: stuff first, st but it's fun. I'm really excited Hai: Yeah, man, keep me keep me updated. Seems like you're on the right track. Gil: Thank you. Athira Praveen: My AI, like response actually takes a lot of time. I'm not sure if that's Athira Praveen: normal, like, it takes like 2, 3min for the AI response to Hai: Not normal. Athira Praveen: Come on, that's not normal, right? Athira Praveen: Never see something wrong. Hai: Well, I guess depends. Are you using like o 1 pro or something? Athira Praveen: No, I'm using the AI SDK. Athira Praveen: 4 4 4 point Gpt. 4, I think. Hai: No. Athira Praveen: Yeah, at least. Yeah. Hai: So without seeing the code or looking at logs. Can you, if you want to like. Hai: add some tracing to your code, so like, add some logging to like the result. And what happens before the result Athira Praveen: Yeah. Hai: Maybe we can take a look at that together at some point over chat Athira Praveen: Yeah, will it happen if my prompt is too big? Hai: How big I'm talking Athira Praveen: I mean, not that big, I mean, it's Athira Praveen: I don't know how to explain how big it could be, but Hai: It shouldn't matter, because I I pass like a whole repo into Athira Praveen: No, no, no, it's not that big at all. It's like a page. Maybe Hai: Yeah. Athira Praveen: Yeah. Athira Praveen: So when you say, like, yeah, maybe like. Hai: Lock some stuff out into the console, because when you say Hai: AI responds slow like, do you mean like when it shows up on the screen, or when it's like, actually Athira Praveen: No, it's like in the loading state, in the thinking state Hai: Oh, thinking state like you model right? Athira Praveen: When it's just loading like when it's just when you submit, and then before it pops up into your screen like it takes Athira Praveen: 2 to 3min. Let me not 3, 2 and a half Hai: 2 or 3min Athira Praveen: 2min for sure. Meri Nova: What! What is your stack? What are you using Athira Praveen: Next year's typescript. Athira Praveen: Yeah. So yeah, I'm I'm thinking, I should. Yeah, like, I said, add some traceability there. Athira Praveen: Cause? Yeah, because I don't Oren: Maybe add some times for the before and after to see if it improves. Once you do stuff, and Oren: maybe it goes down to 90 seconds. Maybe it goes down to 60, but eventually Athira Praveen: Yeah. Hai: Yeah, actually, you should sign up for Langsmith, and then just add it to your code base. It's really simple, like, it's like one line of code, and then you have, like Openai tracing, and you can see like how long each each call to Openai will take under the latency. Hai: and it's free, for, like 1, 1 million traces or something like that, so it should be free for Athira Praveen: That's good. Yeah. Hai: Yeah. So once we once you set up. So I strongly recommend set up setting up Blanksmith Athira Praveen: And then Hai: Show me the traces, and then we'll we'll see. We'll take it from there Athira Praveen: Thank you. Hai: Deep. Hai: Come on, people, don't be shy Dwayne Joseph: My project is going well. I'm implementing Dwayne Joseph: the part where you can like Dwayne Joseph: kind of have a coding agent for my HTML, Css. Code, editor, and I'm starting that process now. Hai: Nice Dwayne Joseph: Everything is, yeah, everything is looking good. Dwayne Joseph: If I have questions I will let you know Hai: Awesome man love to hear it Ekta: About my progress up until Friday. And now Ekta: so I was able to scrape so I I'm working on Ekta: something like very similar to what Kelsey is doing, but in different domain. And like, it's, it's not very domain specific. It's like you would search any length of topics on the research. Ekta: and then, the AI part would summarize the research for you. Ekta: Re-rank it. And basically, after summaries, it would give you a resource direction. Ekta: For example, if you say that I want to research on X topic and the existing research and the X topic, it would take the Delta and suggest you, what's the research direction? Exactly. How should you work on? And what should you focus on after summarizing the papers that exist in the space? Ekta: So I'm pretty much able to find out all the papers like through different Apis, like not just a semantic scholar, but like pubmed AI generated also trying to use firecrawl like you suggested. Ekta: and then Ekta: it's it's going pretty smooth, and it's and the relevancy is also going pretty smooth like up until Friday I could get top 10 papers, which I'm going to squeeze on top 5, Ekta: and then the the summary part. That's not working. So I'm fixing that because all of the time I was just white coding. Basically, I'm new to typescript. Ekta: So I'm a little embarrassed like I did not Hai: That we're clipping that play that on Linkedin Ekta: So basically doing typescript. Ekta: yeah, typescripting sorry using typescript and learning more of it. And trying to debug where I'm failing for the research summary. Ekta: So right now, I'm stuck at. But the relevancy part is really good, like, it's it's throwing. Yeah. Hai: That's awesome. How do you do your Ekta: Moment for me. What was that sorry Hai: How do you do your re-ranking Ekta: So I just use Claude and Chat gpt for model, like 3.5. Sorry Ekta: and use that formulae. That's the basic formulae that it grows like just a mathematical formula like, How is it calculating the context window? Sorry the tokens of all the Ekta: embeddings? And then it gives you the highest score Ekta: like in percent wise 100%, 78 blah blah. And then Ekta: I'm just getting the numbers based on these models right now. So first, st Claude, Claude was expensive. So I'm just focusing on Gpt. And some free models Hai: And I'm still trying to fine tune and sorry, still trying to optimize on that part. So Ekta: But that was a good like learning for me, like, how is it using the relevancy on the topic Hai: Yeah, that's interesting. How we do ranking like that. Hai: It's cool. If it works, it works Ekta: Yeah, I'm not using vector, dB, and all of that, I was just trying to get paper ids into super base right now. Not created my login page also. Because I just focused on getting, you know, working ui, and Ekta: if I'm actually able to call the Apis etc, from different sources. I'll try to use tool calling for the cross referencing of multiple sources like being very specific for specific domain. If I need scientific papers, just, you know, just go through this. And if you need Ekta: any other technology, papers go through other Api, etc. So Ekta: let's see, how does that work? In my case. Hai: Cool. Hai: I'm excited to see it Hai: is Tess. Here, Tess, Tess, I don't think Tessa's here anymore. Hai: Frederick Hai: Patrick Frederick Z: Yeah, I do have do have questions. But you said you were tired about 30min ago. So I don't wanna delay you. So. Frederick Z: well, okay, fine. So so so basically, I'm working with 2 models right now, like, I have my, I have one Llm for my companion model, and I have another Llm. For my agent. So so what I'm doing is that I'm piping the output of my agent after they they finish their tool, calling back to back to the the the main companion model. Frederick Z: Now, when I test these models individually, they seem to work pretty well. But when I when I combine the like, something something weird is happening like I don't know what lang graph is doing, but like like there's some sort of threading issue, because my main function, like the function that's supposed to only run once when you run streamlet, run it. Frederick Z: Appy. It gets called again and again and again and again. So there's some sort of threading that's go going on that I'm not doing. Frederick Z: That's going on in the background. But but I I guess the other thing is that the the model I'm using for the agent tool calling, which is Quinn? Frederick Z: It's being very stochastic, like. So like, even though when I set the temperature to 0, it's still like sometimes it calls the tools, and other times it won't, even though the inputs the same. So I'm gonna try that thing. I'm gonna like, rename the function like the tool functions. I'm gonna adjust the system prompt. And I'm also Hai: You might need to do some few shot there, because for smaller models like Quinn's models, few shotting usually helps Frederick Z: Okay. Alright, alright, I will keep that in mind. But yeah. Yeah. So I'll see what progress I make by Wednesday, and if not. I'll I'll reach out to you again. Office hours Hai: And there are 2 ways to do a few shouting, too. You can do a few shouting in the system message, so like, just literally say, like, here are some examples, or you can insert fake messages into your message history. That's like my my, go to Frederick Z: Oh! Hai: Yeah. Frederick Z: Oh, okay, so so insert, insert synthetic messages into the history itself. Okay, he's really Hai: I, from my experience, that has been a better performing Hai: way to do it. So like, if you have a tool call message the message. See, you would have like a fake question, and then you have, like the right tool call after that from the assistant, and then just keep doing that for all the tools a couple of times, and then somehow, that's like super super clear for the model to know like, Oh, I should do this when this happens Frederick Z: Huh? Okay, alright. So something to think about. Okay, thanks. Thanks. A lot Hai: Yeah. And for the 1st question, I honestly don't know. Maybe if we can look at the code together or something. But Hai: is your agent running like Async or Sync. Frederick Z: No, no, I'm not doing, Tom what may. Maybe I should look into Async. But right now I'm not like I'm not labeling any of my functions as Async, because I was not expecting multi threading, but because there, it's being called sequentially like it's not like 2 models running at at the same time. It's like, first.st the the agent gets gets the user query, it runs it runs the tool calls like it needs, and then it pipes the output Frederick Z: back into the the main main model. So like like, it's supposed to be done sequentially. So I didn't. Frederick Z: No, I didn't think the need like I needed Async. But Frederick Z: but yeah, well, yeah, we'll we'll see May. Maybe it's not that big of a deal, but we'll we'll see like Hai: Oh, let me ask you something. Are you doing hands off like Hai: through code, or like regular code? Or are you doing it through like land graphs. Api, because land graph has like a Hai: like a node thing that can pass on to another agent. But it has to be like a land graph like method, and supposed to like Frederick Z: I might be doing doing it through through. I'm definitely calling so like a certain language like, I'm building the like. I'm building the graph for for the agent routing protocol like like and then and then the result of that like I'm calling the stream method. So so that that might be it. Frederick Z: Okay. But Frederick Z: but but yeah, like, it's not necessarily problematic, like the output still going on, it's just weird, because I'm not. I'm not supposed to be doing multi threading. It's not something I'm doing on purpose. So we'll see. We'll see. Okay. But yeah, yeah, I'll I'll take her. I'll definitely take your advice into into consideration. Hai: Yeah. Interesting problem. Hai: I would be curious to see what you find. Hai: Cool, alright guys. That's it for the stream today. Tune in tomorrow at one Pm. Est. To see me build couch tasks. Meri Nova: Awesome. See? You guys. Gil: Awesome. Thank you so much. Dwayne Joseph: Good night. Gil: David. Hai: Peace.",
  "duration": 7994.385,
  "sentiment": {
    "overall": 0.4384739387426115,
    "timeline": [
      {
        "timestamp": 0.0,
        "sentiment": 0.5808383233532934
      },
      {
        "timestamp": 900.0,
        "sentiment": 0.39204545454545453
      },
      {
        "timestamp": 1800.0,
        "sentiment": 0.44696969696969696
      },
      {
        "timestamp": 2700.0,
        "sentiment": 0.3698630136986301
      },
      {
        "timestamp": 3600.0,
        "sentiment": 0.43283582089552236
      },
      {
        "timestamp": 4500.0,
        "sentiment": 0.4125
      },
      {
        "timestamp": 5400.0,
        "sentiment": 0.47804878048780486
      },
      {
        "timestamp": 6300.0,
        "sentiment": 0.46621621621621623
      },
      {
        "timestamp": 7200.0,
        "sentiment": 0.5833333333333334
      }
    ]
  },
  "speakers": [
    {
      "name": "Hai",
      "speaking_percentage": 0.0,
      "sentiment": 0.4969450101832994
    },
    {
      "name": "Meri Nova",
      "speaking_percentage": 0.0,
      "sentiment": 0.64
    },
    {
      "name": "kelseydilullo",
      "speaking_percentage": 0.0,
      "sentiment": 1.0
    },
    {
      "name": "Frederick Z",
      "speaking_percentage": 0.0,
      "sentiment": 0.6666666666666666
    },
    {
      "name": "Sagar",
      "speaking_percentage": 0.0,
      "sentiment": 0.7058823529411765
    },
    {
      "name": "Autumn Hicks",
      "speaking_percentage": 0.0,
      "sentiment": 1.0
    },
    {
      "name": "Oren",
      "speaking_percentage": 0.0,
      "sentiment": 0.5882352941176471
    },
    {
      "name": "demetrios dolios",
      "speaking_percentage": 0.0,
      "sentiment": 1.0
    },
    {
      "name": "Dwayne Joseph",
      "speaking_percentage": 0.0,
      "sentiment": 0.8333333333333334
    },
    {
      "name": "Athira Praveen",
      "speaking_percentage": 0.0,
      "sentiment": 0.68
    },
    {
      "name": "Ekta",
      "speaking_percentage": 0.0,
      "sentiment": 0.38461538461538464
    },
    {
      "name": "Gil",
      "speaking_percentage": 0.0,
      "sentiment": 0.7272727272727273
    },
    {
      "name": "Melissa",
      "speaking_percentage": 0.0,
      "sentiment": 1.0
    }
  ],
  "reactions": {
    "reactions": [
      {
        "name": "",
        "count": 14
      },
      {
        "name": "",
        "count": 10
      },
      {
        "name": "",
        "count": 9
      },
      {
        "name": "",
        "count": 4
      },
      {
        "name": "",
        "count": 2
      },
      {
        "name": "",
        "count": 2
      },
      {
        "name": "",
        "count": 1
      },
      {
        "name": "",
        "count": 1
      },
      {
        "name": "",
        "count": 1
      }
    ]
  },
  "participants": {
    "total_participants": 17,
    "active_participants": 17,
    "speaking_participants": 13,
    "reacting_participants": 14
  },
  "summary": "- The conversation revolved around optimizing a recipe generation model.\n   - Discussions included reducing the token count, choosing which parts of the HTML to process, and potential issues with parsing responses.",
  "actionItems": [
    "Optimize the recipe generation model to reduce token count.",
    "Experiment with processing different parts of the HTML for better results.",
    "Improve the parsing of responses to avoid generating unnecessary information."
  ],
  "insights": "Topic Analysis:\n- Recipe Optimization: Discussed strategies to optimize the recipe generation model, particularly focusing on reducing token count. Sentiment: Neutral\n   - HTML Processing: Explored the possibility of processing different parts of the HTML for better results. Sentiment: Neutral\n   - Parsing Responses: Discussed potential issues with parsing responses and the need to improve this process. Sentiment: Neutral\n\nFeedback/Insights:\n- It is essential to find a balance between the amount of data processed and the token count to ensure efficient model performance.\n   - The model might benefit from focusing on specific parts of the HTML for better results, as processing all of it may not be necessary.\n   - Improving the parsing of responses can help avoid generating unnecessary information and improve the quality of the output.",
  "comments": [
    {
      "timestamp": "00:00:57",
      "author": "Matas Rebolledo",
      "message": "Hi there!!",
      "reactions": []
    },
    {
      "timestamp": "00:03:25",
      "author": "Matas Rebolledo",
      "message": "That great!",
      "reactions": []
    },
    {
      "timestamp": "00:07:03",
      "author": "Autumn Hicks",
      "message": "Yay Praveena!",
      "reactions": []
    },
    {
      "timestamp": "00:07:12",
      "author": "Ekta",
      "message": "Praveena so proud of you",
      "reactions": []
    },
    {
      "timestamp": "00:07:25",
      "author": "Autumn Hicks",
      "message": "And Kelsey!!",
      "reactions": []
    },
    {
      "timestamp": "00:07:42",
      "author": "Oren",
      "message": "",
      "reactions": [
        ""
      ]
    },
    {
      "timestamp": "00:07:56",
      "author": "Praveena Suresh",
      "message": "Thank you Meri! Thats very kind of you! Thanks everyone.",
      "reactions": []
    },
    {
      "timestamp": "00:08:07",
      "author": "Dwayne Joseph",
      "message": "Congrats yall!",
      "reactions": []
    },
    {
      "timestamp": "00:08:54",
      "author": "Praveena Suresh",
      "message": "Congrats Kelsey!",
      "reactions": []
    },
    {
      "timestamp": "00:09:30",
      "author": "Hai",
      "message": "https://www.figma.com/board/Xoo19tDsQ8gbW0lAndhfoo/Untitled?node-id=0-1&t=Eiqe54rPlNYXOdrI-1",
      "reactions": []
    },
    {
      "timestamp": "00:13:43",
      "author": "Frederick Z",
      "message": "I am using an open source LLM for my agent... and it is not fun!",
      "reactions": []
    },
    {
      "timestamp": "00:17:30",
      "author": "Frederick Z",
      "message": "Here are a few of my tools. I wrote them myself (doesn't have to be an external API).",
      "reactions": []
    },
    {
      "timestamp": "00:49:58",
      "author": "Gil",
      "message": "This is exactly what I need for the model to spit out the DSL content in a standardized and reliable way for future integrations with other tools.",
      "reactions": []
    },
    {
      "timestamp": "00:50:36",
      "author": "Athira Praveen",
      "message": "This is what I was looking for ! Haha dint even know this existed.",
      "reactions": []
    },
    {
      "timestamp": "00:51:05",
      "author": "Oren",
      "message": "I was literally working with AI Studio without realizing its right there.",
      "reactions": []
    },
    {
      "timestamp": "00:51:06",
      "author": "Gil",
      "message": "Im giving the model the JSON spec within a prompt template. Probably not a reliable approach.",
      "reactions": []
    },
    {
      "timestamp": "00:51:17",
      "author": "Praveena Suresh",
      "message": "Yes I tried Hai!",
      "reactions": []
    },
    {
      "timestamp": "00:51:20",
      "author": "Gil",
      "message": "I did, pretty cool tool.",
      "reactions": []
    },
    {
      "timestamp": "00:55:16",
      "author": "Sagar",
      "message": "@Hai",
      "reactions": []
    },
    {
      "timestamp": "01:03:14",
      "author": "Ekta",
      "message": "Thanks for asking",
      "reactions": []
    },
    {
      "timestamp": "01:03:16",
      "author": "Ekta",
      "message": "Sagar",
      "reactions": []
    },
    {
      "timestamp": "01:04:13",
      "author": "Athira Praveen",
      "message": "YES!!",
      "reactions": []
    },
    {
      "timestamp": "01:04:22",
      "author": "Praveena Suresh",
      "message": "Yes please",
      "reactions": []
    },
    {
      "timestamp": "01:04:22",
      "author": "Gil",
      "message": "Lets go!",
      "reactions": []
    },
    {
      "timestamp": "01:05:49",
      "author": "Gil",
      "message": "There are also alternatives like Miro and Mural.",
      "reactions": []
    },
    {
      "timestamp": "01:06:20",
      "author": "Gil",
      "message": "Make money!",
      "reactions": []
    },
    {
      "timestamp": "01:07:30",
      "author": "Frederick Z",
      "message": "Get that",
      "reactions": [
        ""
      ]
    },
    {
      "timestamp": "01:17:38",
      "author": "Autumn Hicks",
      "message": "I found this tool for using multiple APIs for search functions: https://serpapi.com/                       Might be helpful in someone else's project! I'm using it to search Yelp and Bing without needing separate APIs",
      "reactions": []
    },
    {
      "timestamp": "01:41:14",
      "author": "Autumn Hicks",
      "message": "I have to drop, thanks Hai!!",
      "reactions": []
    },
    {
      "timestamp": "01:41:18",
      "author": "Athira Praveen",
      "message": "YAYYY",
      "reactions": []
    },
    {
      "timestamp": "01:41:40",
      "author": "Athira Praveen",
      "message": "Thanks again Hai!",
      "reactions": []
    },
    {
      "timestamp": "01:42:47",
      "author": "Gil",
      "message": "Agreed!",
      "reactions": []
    },
    {
      "timestamp": "01:43:17",
      "author": "Frederick Z",
      "message": "I'll wait for Wednesday's office hours to ask my questions. Hai needs rest.",
      "reactions": []
    },
    {
      "timestamp": "01:43:46",
      "author": "Gil",
      "message": "Thanks, always fun!",
      "reactions": []
    },
    {
      "timestamp": "02:04:45",
      "author": "Frederick Z",
      "message": "If latency is greater than 30s, it's considered unacceptable for prod.",
      "reactions": []
    },
    {
      "timestamp": "02:11:32",
      "author": "Ekta",
      "message": "ok gott hop off",
      "reactions": []
    },
    {
      "timestamp": "02:14:19",
      "author": "Gil",
      "message": "Wow, thats a cool tip!",
      "reactions": []
    }
  ]
}